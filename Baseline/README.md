# Fact Extraction and VERification

This is an implementation of the [FEVER shared task](http://fever.ai). The goal is to create a system based on a large corpus which determines whether a given claim is supported, refuted, or with not enough information for factual verification. We use pre-processed Wikipedia Pages (June 2017 dump) as the evidence corpus and this is provided by the FEVER task, together with the large training dataset with 185,445 claims generated by altering sentences extracted from Wikipedia. The dataset is labeled as Supported, Refuted, and NotEnoughInfo with necessary evidences for the judgenment. We employ TF-IDF approach to the term-document frequency matrix to retrieve most relevant documents and sentences. Simple linear classification with overlapping words results in 37\% accuracy, and it is comparable to the results of the baseline approach in the original FEVER paper.


## Installation

Clone the repository


    pip install -r requirements.txt

Download the FEVER dataset from the [website](http://fever.ai/data.html) into the data directory

    mkdir data
    mkdir data/fever-data
    
    # We use the data used in the baseline paper
    wget -O data/fever-data/train.jsonl https://s3-eu-west-1.amazonaws.com/fever.public/train.jsonl
    wget -O data/fever-data/dev.jsonl https://s3-eu-west-1.amazonaws.com/fever.public/paper_dev.jsonl
    wget -O data/fever-data/test.jsonl https://s3-eu-west-1.amazonaws.com/fever.public/paper_test.jsonl
    

## Data Preparation
The data preparation consists of three steps: downloading the articles from Wikipedia, indexing these for the Evidence Retrieval and performing the negative sampling for training.  

### 1. Download Wikipedia data

Download the pre-processed Wikipedia articles from [the website](https://sheffieldnlp.github.io/fever/data.html) and unzip it into the data folder.
    
    wget https://s3-eu-west-1.amazonaws.com/fever.public/wiki-pages.zip
    unzip wiki-pages.zip -d data
 

### 2. Construct SQLite Database
Construct an SQLite Database. A commercial personal laptop seems not work when dealing with the entire database as a single file so we split the Wikipedia database into a few files too. 
    
    python build_db.py data/wiki-pages data/single --num-files 1
    python build_db.py data/wiki-pages data/fever --num-files 5


### 3. Create Term-Document count matrices and merge
Create a term-document count matrix for each split, and then merge the count matrices.
    
    python build_count_matrix.py data/fever data/index
    python merge_count_matrix.py data/index data/index


### 4. Reweight the count matrix
Two schemes are tried, TF-IDF and PMI.
    
    python reweight_count_matrix.py data/index/count-ngram\=1-hash\=16777216.npz data/index --model tfidf


The remaining task for FEVER challenge, i.e. document retrieval, sentence selection, sampling for NotEnoughInfo, and RTE training are done in IPython notebook `fever.ipynb` and implementation in `fever.py`. The class `Oracle` reads either TF-IDF or PMI matrix and have methods for finding relevant documents, sentences, etc. given the input claim.


